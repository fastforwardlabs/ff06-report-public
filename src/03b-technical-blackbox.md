### Black-box Interpretability

If you won't or can't change your model, or you didn't make it and don't have
access to its internals, white-box approaches are not useful. In this extremely
common situation, you need an approach that allows you to interpret a black-box
model. Thanks to recent research, this is not only possible, but relatively
simple.

![FIGURE 3.12 New techniques can make highly accurate neural network algorithms much more interpretable.](figures/3-11.png)

Until recently, the usual way to interpret a black-box model was to train two
models: the uninterpretable, highly accurate model that you use in production,
and a _shadow_ interpretable model you use solely to learn about the system.
The shadow model is trained not on real training data, but on simulated data
generated by the uninterpretable, accurate model. It's therefore a caricature
of the truth and, at a high level, may capture some of the broad strokes of the
model. By inspecting the interpretable shadow model, you can offer
explanations.

![FIGURE 3.13 A shadow model can be trained from more complex models.](figures/3-04.png)

The problem is that the shadow model is not only simplistic by construction.
Its explanations can be misleading in ways you have no easy way of knowing.
Inferences drawn from it are dubious, and come with no statistical guarantees
about how wrong they could be. Simply put, you can offer explanations for the
production model's decisions, but you have no way of knowing if those
explanations are correct.^[One (anonymous) data scientist told us the
interpretable twin model they use to explain their production model to clients
is no better than "shadows on the cave wall."] Additionally, you now need to
build and maintain two models, which can be a significant engineering burden.

#### Perturbation

Perturbation is a model-agnostic interpretability technique that requires you
to build and maintain only one model. This is your production model. It can be
as complicated and uninterpretable as is justified by your dataset and
performance requirements. Despite this, the strategy offers a more faithful
description of the uninterpretable model than the interpretable shadow model
technique described in the previous section.

The basic idea is simple, and is probably exactly what you'd do if you were
asked to figure out how a black-box system worked. The input is _perturbed_,
and the effect of that perturbation on the output is noted. This is repeated
many times, until a local understanding of the model is built up.

![FIGURE 3.14  By perturbing feature inputs, a local understanding of the model can be built up.](figures/3-05.png)

Let's look at a real-world example of the application of this basic idea in its
simplest, most manual form. The website Credit Karma offers a Credit Score
Simulator. At first the simulator shows the user their current score. The user
can then change one of the two dozen or so inputs, to see what the effect is.
The natural thing to do is to try changing them all, one at a time, to see
which has the biggest effect.

![FIGURE 3.15 Credit Karma lets users see how changes affect their credit score.](figures/3-06.png)

Credit score is a nonlinear model; two people can open the same new credit card
and it can have very different effects on their credit score. This means it is
impossible to summarize the model _globally_ by saying something like *"Lose 10
points per open credit card."* But if a particular user of the Credit Score
Simulator discovers their score goes down by 10 points when they propose
opening a new credit card, that is a valid explanation of the behavior of the
model _locally_, in the vicinity of that user in feature space.

#### LIME

Local Interpretable Model-agnostic Explanation (LIME)^[Ribeiro, Singh,
and Guestrin (2016), [*"'Why Should I
Trust You?': Explaining the Predictions of Any Classifier."*](https://arxiv.org/abs/1602.04938)] formalizes the
perturbation technique described in the previous section. It's exciting because
it provides a simple method to interpret arbitrary black-box models. The
algorithm is computationally simple, and the public reference implementation is
a drop-in addition to many machine learning pipelines.

![Figure 3.16 LIME perturbs features to find a local linearly interpretable space.](figures/3-07.png)

LIME takes as input a trained model and the particular example whose
classification you want to explain. It then randomly perturbs the features of
the example, and runs these perturbed examples through the classifier. This
allows it to probe the surrounding feature space and build up a picture of the
classification surface nearby.

It probes the classifier's behavior in this way a few thousand times, and then
uses the results as training data to fit a linear model. The training
examples are weighted by distance to the original example. The linear model can
then be interpreted as usual to extract explanations like "You will decrease
your credit score by 10 points if you open a credit card." These explanations
are locally faithful; i.e., they are applicable in the region near the original
example.

In a way, this approach is similar to the shadow model approach: the "true"
model is used to generate training data for a simpler, interpretable model. But
while the shadow model offers a supposedly global explanation that is wrong in
unknown ways, LIME offers a local explanation that is correct.

LIME is an exciting breakthrough. It's an extremely simple idea (the
preceding explanation glosses over mathematical detail, but is conceptually
complete). It allows you to train a model in any way you like and still have
an answer to the local question, "Why has this particular decision been made?"
We used LIME to build the [prototype](#prototype) for this report.

#### Extensions and Limitations

LIME is well suited to tabular data. It perturbs categorical features by
sampling from their distribution in the training data, and it perturbs
continuous features by sampling from a normal distribution.

How to meaningfully perturb unstructured input such as images or text is less
obvious. For text, the reference implementation offers two perturbation
strategies. It can either delete words, or replace them with an unknown token.
Using these strategies, the "local" region around the example is the set of
trial documents made by omitting words from the original. By running these
trial documents through the black-box classifier, LIME can learn which words in
the original document are "responsible" for the original classification, and
assign them quantitative importances.

These importances can be used to label the words in a document that are
"responsible" for its classification (by topic, sentiment, etc.). Assuming the
model is accurate, it is presumably relying on some of the same things a human
reader is looking for. If you're the author of the text, you might therefore
find it useful to know which parts of your writing are attracting the model's
attention. If you're the creator of the model, you might find it reassuring (or
alarming) to learn the words your model depends upon.

We applied LIME to a black-box text classifier and saw sensible results. The
model, a recurrent neural network to classify text as clickbait or not, was
truly a black box to
us.^[[https://github.com/saurabhmathur96/clickbait-detector](https://github.com/saurabhmathur96/clickbait-detector)] We found it
online and deliberately avoided reading about its structure. Nevertheless, we
were able to use LIME to probe it, and build up some trust that it was paying
attention to reasonable words.

![FIGURE 3.17 LIME word explanations of the clickbaitiness of headlines.](figures/3-12.png)

The image perturbation strategy suggested by the creators of LIME is
qualitatively similar. The image is divided up into high-level "superpixels,"
which are zeroed out at random during perturbation. The result is the same: an
explanation that says which part of the image is responsible for the behavior of
the black-box model.^[[Fong and
Vedaldi (2017)](https://arxiv.org/abs/1704.03296) recently proposed an image perturbation strategy that results in
even better "explanations."]

![FIGURE 3.18 LIME superpixel explanations of the classification of an image of a dog playing a guitar. Figure and example from LIME paper [https://arxiv.org/abs/1602.04938](https://arxiv.org/abs/1602.04938).](figures/3-08.png)

While LIME is designed with local explanation in mind, with enough explanations
in hand, you can begin to build up in your head a global picture of the model.
But doing this is like playing Battleship: you try examples at random, and
dwell in interesting places when you find them. The creators of LIME also
introduced SP-LIME, an algorithm to select a small number of well-chosen real
examples from a dataset. The algorithm greedily selects examples whose
explanations are as different as possible from each other. The result is a
small number of examples that, along with their explanations, give the big
picture.^[The code is not yet in the reference implementation of LIME,
but can be found at
[https://github.com/marcotcr/lime-experiments/blob/master/compare_classifiers.py](https://github.com/marcotcr/lime-experiments/blob/master/compare_classifiers.py).]

Finally, it's important to note that the LIME approach has fundamental
limitations. Whether it is used to explain a classifier of tabular, text, or
image data, LIME gives explanations in terms of the raw input features. If
those features are not interpretable, then LIME will not help. For example, if
the initial input to a model is an uninterpretable text embedding, LIME will
not offer an explanation that makes sense to humans.

Also, as with any attempt to attribute causal relationships to data, there are
dangers of confusing correlation and causation. This risk, which exists
throughout machine learning, is equally if not more acute when using LIME. It
is easy to read LIME's explanations as saying things like "This cable customer
is going to churn _because_ they do not have TV service," which may be a
misinterpretation. The risk of this is highest when when the supposed causal
relationship seems to confirm your expectations.


::: info
##### *Global black-box interpretation with FairML*

FairML is an open source tool released by Julius Adebayo when he was a member
of the Fast Forward Labs team. It is similar to LIME in the sense that it
probes a black-box model by perturbing input, but it provides a single global
interpretation that assigns an importance to each feature. As with a shadow
model, this may gloss over important details, but for the purposes of auditing
a model for harmful global biases it is a great tool. For example, it can be
used to measure the extent to which a model depends on "protected features"
that, from a legal and ethical point of view, should make no difference to its
output (see <<ethics>>). A more detailed introduction to FairML is available
on the Fast Forward Labs blog.^[[http://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html](http://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html)]

:::

#### SHAP

Following the release of LIME, there has been continuous research focused on improving accuracy
as well as the user experience tooling for explaining blackbox models. One such contribution, 
which has rapidly become an widely used, is SHAP ^[[https://github.com/slundberg/shap](SHAP) SHAP: A game theoretic approach to explain the output of any machine learning model.]. SHAP stands for SHapley Additive exPlanations and its primary contribution is the introduction of a game theoretic foundation (Shapley values) for assigning feature importance values for each prediction produced by a model. [Shapley values](https://en.wikipedia.org/wiki/Shapley_value) (introduced by Lloyd Shapley in 1953) are a method for fairly assigning credit to participants in a cooperative multiplayer game based on their contributions to the overall game outcome.

To compute the Shapley value for a given player, we compute each outcome where the player was present and compare it to the outcome where they were not present. For a game consisting of N players, there is a large surface of outcome combinations (N!) where each player is present or absent, making the computation of Shapley values computationally expensive.

 More importantly, the Shapley value approach for assigning credit is _fair_ because it adheres to a list of certain mathematical properties (Efficiency, Symmetry, Linearity, Anonymity, Marginalism)  beyond the scope of this writeup. When applied to model explanations (assigning credit for each feature in a prediction), the integration of the Shapley approach yields two valuable properties 
- local accuracy (an approximate model used to explain the original model should match the output of the original model for a given input)
- consistency (if the original model changes such that a feature has a larger impact in every possible ordering, then its attribution should not decrease)

In their paper, ^[[https://arxiv.org/pdf/1705.07874.pdf](SHAP) https://arxiv.org/pdf/1705.07874.pdf] the authors of SHAP show that most approaches to explaining black box models (LIME, DeepLIFT, Relevance Propagation) can be categorized as additive feature attribution methods and that a Shapley value approach is the only approach that guarantees the local accuracy and consistency properties within the category. The authors also show through experiments how these properties help to avoid unintuitive results that can sometimes be observed with non-shapely methods like LIME.

SHAP is implemented as a python library with an easy to use interface and a set of useful visualizations. To address the complexity of computing Shapley values, the authors implement optimizations that take advantage of the structure of specific models. For example, the SHAP `TreeExplainer` is optimized for tree based models (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models) while `DeepExplainer` and `GradientExplainer` are optimized for neural networks. However, SHAP remains slow when used in model agnostic mode (`KernelSHAP`).

The code below shows how to generate a list of Shapley values that quantify the effect of each feature on the model prediction.

::: info
    import shap
    # model is a sklearn decision tree model applied to the churn dataset used in the prototype
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test)

:::

The SHAP python library also provides helpul visualizations that further illustrate the global and local impact of each feature.

::: info
    # Local explanation for single instance
    shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:], matplotlib=True) 

    # Global explanation for all instances in test set
    shap.summary_plot(shap_values, X_test)
    shap.summary_plot(shap_values, X_test, plot_type="bar")
:::


![FIGURE 3.18 The SHAP library provides the `force_plot` visualization which shows the local feature importance for each a given data instance.](figures/3-Shap3.png)

![FIGURE 3.19 The SHAP library provides the `summary_plot` visualization which shows the global feature importance entire test set.](figures/3-Shap1.png)

![FIGURE 3.20 The SHAP library provides the summary_plot visualization (type=bar) which shows the global feature importance for entire test set.](figures/3-Shap2.png)
   

